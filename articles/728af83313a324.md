---
title: "研究室で余ってる計算機を活用してGPUクラスタを構築してみた with kubernetes"
emoji: "✌️"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["機械学習", "kubernetes", "DDP", "分散学習"]
published: false
---

# 1. 概要

本記事では、kubernetes を用いて、オンプレミスの計算機サーバー間で分散学習を行う方法・手順について紹介します。

## 環境

- master(control plane)１台 × worker ２台でクラスタを構築します。
  (簡略化のために最小構成にしてます)
- master, worker 共に ubuntu22.04 を使用
  - GPU は NVIDIA を使用

# 2. master ノードのセットアップ

## Kubernetes の動作要件を満たすように設定

指定したカーネルモジュール(overlay, br_netfilter)をシステム起動時に自動的にロードされるようにする

```
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter
```

kubernetes のネットワーク設定

```
cat <<EOF | sudo tee /etc/sysctl.d/kubernetes.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sudo sysctl --system
```

swap を無効にする

```
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```

## NVIDIA やコンテナ関連のコンポーネントのインストールと containerd の設定

1. NVIDIA driver のインストール
2. docker, containerd のインストール
3. nvidia-container-runtime のインストール、`/etc/docker/daemon.json`より、ランタイムオプションを設定

```
{
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}
```

4. containerd の config 設定

   - デフォルトランタイムの設定

     ```
     sudo apt-get install -y nvidia-container-toolkit

     sudo containerd config default | sudo tee /etc/containerd/config.toml
     sudo nvidia-ctk runtime configure --runtime=containerd
     ```

     `/etc/containerd/config.toml`より、`default_runtime_name`を`nvidia`に設定する

   - systemd バックエンドな cgroup を有効にする
     `/etc/containerd/config.toml`より、 nvidia runtime options の書き換え

     ```
     [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
        省略...
        SystemdCgroup = true
     ```

   - containerd 再起動
     `sudo systemctl restart containerd.service`

※NVIDIA driver 設定後は再起動が必要

## kubernetes 関連コンポーネントのインストール

kubelet, kubeadm, kubectl のインストール

```
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key |sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

## kubeadm で Kubernetes クラスタを構築

- kubeadm は実用最小限の Kubernetes クラスターを作成してくれる。

### kubeadm init でクラスタ初期化

オプションの`--pod-network-cidr`で Pod に割り当てる IP アドレスの範囲を指定する。
今回は以降でコンテナ間通信の仮想ネットワークとして flannel を使用するため、flannel のデフォルトネットワーク`10.244.0.0/16`に合わせる。

```
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
```

kubeadm init 後の output に worker がクラスタに参加するためのコマンド(`kubeadm join ...`)が表示されるので控えておく。

- kube-api に接続するために以下を実行する

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

- `kubectl get node`でノードの状態が確認できれば OK。（この時点ではまだネットワーク関連の設定が完了していないため、マスターノードは NotReady 状態となっている）

## flannel で仮想ネットワークを構築

- 設定ファイルを GitHub から取得し、実行する

```
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

kubectl apply -f kube-flannel.yml
```

`kubectl get node`でノードが Ready になっていることを確認する

&nbsp;
:::details 参考サイト

- nvidia 周りの設定

https://qiita.com/ttsubo/items/c97173e1f04db3cbaeda

- kubernetes の設定周り

https://knowledge.sakura.ad.jp/20955/

https://qiita.com/ohtsuka-shota/items/7618cd98a12c8b2e4e07
:::

# 3. worker ノードのセットアップ

### master ノードと同様にセットアップを行う

- [kubernetes-の動作要件を満たすように設定](#kubernetes-の動作要件を満たすように設定)
  [NVIDIA、コンテナ関連のコンポーネントのインストール & containerd の設定](#nvidia-やコンテナ関連のコンポーネントのインストールと-containerd-の設定)
  [kubernetes 関連コンポーネントのインストール](#kubernetes-関連コンポーネントのインストール)

### クラスタに参加

- `kubeadm join`コマンドを実行する
  - トークンには有効期限があり、通常は 24 時間で利用できなくなるらしい
  - 失効した場合は、master ノードで`kubeadm token create --print-join-command`を実行すると再発行できる
- master ノードで`kubectl get node`を実行し、worker ノードがクラスタに参加したことを確認する

# 4. GPU-device-driver で worker の GPU を認識できるようにする

# 5. 分散学習用のマニフェストを作成

# 6.まとめ

kubernete を用いて GPU クラスタを作成し、分散学習を行う環境を構築しました。
今後は、分散学習における学習プロセスの最適化や、機械学習フローの自動化、クラスタ構成の冗長化等にチャレンジしてみたいと思います。
